# -*- coding: utf-8 -*-
"""cap13_cont.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LYEBh5h9dVyOzuI22QpXqcRX_YInzH3E
"""

import numpy as np


def softmax(x):
       # Subtrai o máximo para estabilidade numérica
    x = np.clip(x, -50, 50)
    x_shift = x - np.max(x, axis=-1, keepdims=True)
    exp_x = np.exp(x_shift)
    return exp_x / (np.sum(exp_x, axis=-1, keepdims=True) + 1e-7)


def softmax_prime(x):
    return softmax(x) * (1 - softmax(x))

def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

def leaky_relu_prime(x, alpha=0.01):
    return np.where(x > 0, 1, alpha)

def relu(x):
  return np.maximum(0, x)

def relu_prime(x):
  return (x > 0).astype(float)

def identity(x):
  return x

def identity_prime(x):
  return np.ones(x.shape)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_prime(x):
    return sigmoid(x) * (1 - sigmoid(x))

def grad_loss(y, y_pred):
  return (y_pred - y)**2

def grad_loss_prime(y, y_pred):
    return 2*(y_pred - y)

def binary_cross_entropy(y_true, y_pred):
    return - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

def binary_cross_entropy_prime(y_true, y_pred):
    return (y_pred - y_true) / (y_pred * (1 - y_pred) + 1e-6)

def categorical_cross_entropy(y_pred, y_true):
    return -np.sum(y_true * np.log(y_pred + 1e-8)) / y_true.shape[0]

def categorical_cross_entropy_prime(y_pred, y_true):
    return -(y_pred - y_true)

def tanh(x):
    return np.tanh(x)

def tanh_prime(x):
    return 1 - np.tanh(x) ** 2
#Ian Goodfellow
class mlp():
    def __init__(self, hidden, grad_loss, grad_loss_prime, hidden_func, hidden_func_prime, output_funcs, output_funcs_prime, epochs=1000, eta=0.01):
        self.hidden = hidden
        self.epochs = epochs
        self.eta = eta
        self.size = len(hidden)
        self.weights = [np.random.randn(y, x) * np.sqrt(2 / x) for x, y in zip(self.hidden[:-1], self.hidden[1:])]
        self.biases = [np.zeros((y, 1)) for y in self.hidden[1:]]
        self.hidden_func = hidden_func
        self.hidden_func_prime = hidden_func_prime
        self.output_funcs = output_funcs
        self.output_funcs_prime = output_funcs_prime
        self.grad_loss = grad_loss
        self.grad_loss_prime = grad_loss_prime
  #algorithm 6.3, Ian Goodfellow

    def feedforward(self, x):
      #lets memoize, we need to know the x and f_i(x)
        act = [x]
        f = [x]
        for i, (w, b) in enumerate(zip(self.weights, self.biases)):

            x = np.dot(w, x) + b
            act.append(x)
            if i == len(self.weights) - 1:
                x = self.output_funcs(x)
            else:
                x = self.hidden_func(x)
            f.append(x)
        return act, f

  #algorithm 6.3, Ian Goodfellow
    def backprop(self, x, y):
        act, f = self.feedforward(x)
        w_grads = [np.zeros(w.shape) for w in self.weights]
        b_grads = [np.zeros(b.shape) for b in self.biases]


        g = self.grad_loss_prime(y, f[-1])*self.output_funcs_prime(act[-1])

        b_grads[-1] = g
        w_grads[-1] = np.dot(g, f[-2].T)


        for i in range(2, self.size):
            g = np.dot(self.weights[-i+1].T, g) * self.hidden_func_prime(act[-i])
            b_grads[-i] = g
            w_grads[-i] = np.dot(g, f[-i-1].T)

        return w_grads, b_grads
    
    def fit(self, X, y, batch_size=128):
        X = np.array(X, dtype=np.float32)
        y = np.array(y, dtype=np.float32)

        for epoch in range(self.epochs):
            indices = np.random.permutation(len(X))
            for i in range(0, len(X), batch_size):
              #apply mini_batchs?
              #minibatch alg: 8.4, Deep Learning, Ian Goodfellow
                batch_indices = indices[i:i+batch_size]
                X_batch = X[batch_indices]
                y_batch = y[batch_indices]
                
                w_grads_total = [np.zeros_like(w) for w in self.weights]
                b_grads_total = [np.zeros_like(b) for b in self.biases]
                for x, y__ in zip(X_batch, y_batch):
                    x = x.reshape(-1, 1)
                    y__ = y__.reshape(-1, 1)
                    w_grads, b_grads = self.backprop(x, y__)
                    for i in range(len(w_grads_total)):
                        w_grads_total[i] += w_grads[i]
                        b_grads_total[i] += b_grads[i]
                self.weights = [w - self.eta * (dw/len(X_batch)) for w, dw in zip(self.weights, w_grads_total)]
                self.biases = [b - self.eta * (db/len(X_batch)) for b, db in zip(self.biases, b_grads_total)]

    def predict(self, x):
        x = np.array(x, dtype=np.float32).reshape(-1, 1)
        for i, (w, b) in enumerate(zip(self.weights, self.biases)):
            x = np.dot(w, x) + b
            if i == len(self.weights) - 1:
                x = self.output_funcs(x)
            else:
                x = self.hidden_func(x)
        return x.flatten()

from sklearn import datasets
from sklearn.model_selection import train_test_split

def one_hot_encoding(y, num):
    return np.eye(num)[y]

iris = datasets.load_iris()

data = iris['data']
targets = iris['target']
X_train, X_test, y_train, y_test = train_test_split(

    data, targets, test_size=0.33, random_state=2909)

mean = np.mean(X_train)
std = np.std(X_train)
X_train = (X_train - mean) / std
X_test = (X_test - mean) / std

print(X_train)

y_train = one_hot_encoding(y_train, 3)
y_test = one_hot_encoding(y_test, 3)

mlp_ = mlp([4, 5, 4, 3], grad_loss, grad_loss_prime, leaky_relu, leaky_relu_prime, softmax, softmax_prime, epochs=4000, eta = 0.001)
mlp_.fit(X_train, y_train)

print("----------------------------------------------------------------------------------------------")

acertou = 0

for i, x in enumerate(X_test):
    pred_ = np.argmax(mlp_.predict(x))
    y_real = np.argmax(y_test[i])
    if pred_ == y_real: acertou += 1
    print("predizido: ", pred_)
#    print("real: ", y_real)

print("Acurácia: ", (100*acertou/len(y_test)))


